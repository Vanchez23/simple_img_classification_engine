diff --git a/model/__pycache__/create_dataset.cpython-38.pyc b/model/__pycache__/create_dataset.cpython-38.pyc
index d45cb60..21fc43c 100644
Binary files a/model/__pycache__/create_dataset.cpython-38.pyc and b/model/__pycache__/create_dataset.cpython-38.pyc differ
diff --git a/model/__pycache__/pipeline.cpython-38.pyc b/model/__pycache__/pipeline.cpython-38.pyc
index f58cb90..35d104d 100644
Binary files a/model/__pycache__/pipeline.cpython-38.pyc and b/model/__pycache__/pipeline.cpython-38.pyc differ
diff --git a/model/__pycache__/trainer.cpython-38.pyc b/model/__pycache__/trainer.cpython-38.pyc
index 440f2bb..b6b9018 100644
Binary files a/model/__pycache__/trainer.cpython-38.pyc and b/model/__pycache__/trainer.cpython-38.pyc differ
diff --git a/model/create_dataset.py b/model/create_dataset.py
index c33254e..644e8c4 100644
--- a/model/create_dataset.py
+++ b/model/create_dataset.py
@@ -49,9 +49,9 @@ class CustomDataset(Dataset):
     @staticmethod
     def split_df(df:pd.DataFrame, random_state=2022) -> List[pd.DataFrame]:
 
-        assert 'label' in df.columns
+        assert 'class' in df.columns
 
-        df_train, df_valid = train_test_split(df, test_size = 0.2, stratify=df['label'], random_state=random_state)
-        df_valid, df_test = train_test_split(df_valid, test_size = 0.5, stratify=df_valid['label'], random_state=random_state)
+        df_train, df_valid = train_test_split(df, test_size = 0.2, stratify=df['class'], random_state=random_state)
+        df_valid, df_test = train_test_split(df_valid, test_size = 0.5, stratify=df_valid['class'], random_state=random_state)
 
         return df_train, df_valid, df_test
diff --git a/model/pipeline.py b/model/pipeline.py
index cfbe7c0..15f0dfa 100644
--- a/model/pipeline.py
+++ b/model/pipeline.py
@@ -1,6 +1,6 @@
 from logging import warning
 from typing import Dict, Tuple
-
+from copy import deepcopy
 
 from torch.utils.data import DataLoader
 from pytorch_lightning import Trainer
@@ -18,13 +18,13 @@ class Pipeline:
 
     def __init__(self, cfg:Dict):
 
-        self._cfg = cfg
+        self._cfg = deepcopy(cfg)
         trainer_cfg = self._cfg['trainer']
         logger = WandbLogger(**trainer_cfg['wandb_logger'])
         self.trainer = Trainer(gpus = trainer_cfg['gpus'], 
                         deterministic= trainer_cfg['deterministic'],
                         logger=logger)
-        self.classifier = Classifier(self._cfg['num_classes'])
+        self.classifier = Classifier(self._cfg)
 
 
     def _set_dataloaders(self) -> Tuple[DataLoader,DataLoader, DataLoader]:
@@ -74,10 +74,9 @@ class Pipeline:
 
     def run(self):
         
-        train_loader, valid_loader, test_loader = self._set_dataloaders()
-        self.trainer.fit(self.classifier, train_dataloaders=train_loader,
-                    val_dataloaders=valid_loader)
-        result = self.trainer.test(self.classifier, dataloaders=test_loader)
+        # train_loader, valid_loader, test_loader = self._set_dataloaders()
+        self.trainer.fit(self.classifier)
+        result = self.trainer.test(self.classifier)
 
 
         
diff --git a/model/trainer.py b/model/trainer.py
index 72d3423..7153c14 100644
--- a/model/trainer.py
+++ b/model/trainer.py
@@ -1,25 +1,76 @@
+from typing import Tuple, Dict
+from copy import deepcopy
 import torch
 from torch import nn
+from torch.utils.data import DataLoader
 from torchvision import models
 import pytorch_lightning as pl
+import albumentations as A
+from albumentations.pytorch import ToTensorV2
+
+from create_dataset import CustomDataset
 
 class Classifier(pl.LightningModule):
     
-    def __init__(self, num_classes):
+    def __init__(self, cfg: Dict):
         super().__init__()
+        self._cfg = deepcopy(cfg)
+        self.num_classes = self._cfg['num_classes']
+        self.img_dir = self._cfg['img_dir']
+
+        (self.df_train, 
+        self.df_valid, 
+        self.df_test) = self._split_df(self._cfg['annotation_file'],
+                                        self._cfg['split_dataset'], 
+                                        self._cfg['random_state'])
+
         self.model = models.resnet50(pretrained="imagenet")
         in_features = self.model.fc.in_features
-        self.model.fc = nn.Linear(in_features, num_classes, bias=True)
+        self.model.fc = nn.Linear(in_features, self.num_classes, bias=True)
         self.loss_func = torch.nn.CrossEntropyLoss()
         self.save_hyperparameters()
 
-    def compute_metrics(self, pred, y):
-        accuracy = torch.sum(y == pred).item() / (len(y) * 1.0)
-        return {'accuracy': accuracy}
+################# Dataloaders
+
+    def train_dataloader(self):
+        transform = A.Compose([
+            A.HorizontalFlip(p=0.5),
+            A.RandomBrightnessContrast(p=0.2),
+            A.Normalize(mean=(0.485, 0.456, 0.406), 
+                        std=(0.229, 0.224, 0.225)),
+            A.Resize(256,256),
+            ToTensorV2()
+        ])
+        train_dataset = CustomDataset(self.df_train, self.img_dir, transform)
+        train_loader = DataLoader(train_dataset, batch_size = self._cfg['batch_size'], shuffle=True)
+        return train_loader
+
+    def val_dataloader(self):
+        transform = A.Compose([
+            A.Normalize(mean=(0.485, 0.456, 0.406), 
+                        std=(0.229, 0.224, 0.225)),
+            A.Resize(256,256),
+            ToTensorV2()
+        ])
+        valid_dataset = CustomDataset(self.df_valid, self.img_dir, transform)
+        valid_loader = DataLoader(valid_dataset, batch_size = self._cfg['batch_size'], shuffle=True)
+        return valid_loader
+
+    def test_dataloader(self):
+        transform = A.Compose([
+            A.Normalize(mean=(0.485, 0.456, 0.406), 
+                        std=(0.229, 0.224, 0.225)),
+            A.Resize(256,256),
+            ToTensorV2()
+        ])
+        test_dataset = CustomDataset(self.df_test, self.img_dir, transform)
+        test_loader = DataLoader(test_dataset, batch_size = self._cfg['batch_size'], shuffle=True)
+        return test_loader
+
+################ Steps ############################
 
     def training_step(self, batch, batch_idx):
         x,y = batch
-        # x = x.view(x.size(0), -1)
         pred = self.model(x)
         loss = self.loss_func(pred, y)
         self.log('train_batch_loss', loss)
@@ -46,11 +97,16 @@ class Classifier(pl.LightningModule):
 
     def validation_step(self, batch, batch_idx):
         x,y = batch
-        # x = x.view(x.size(0), -1)
         pred = self.model(x)
         loss = self.loss_func(pred, y)
         self.log('valid_batch_loss', loss)
-        return {'valid_batch_loss': loss}
+
+        pred = torch.argmax(pred, dim=1)
+        metrics = self.compute_metrics(pred, y)
+        for metric_name, value in metrics.items():
+            self.log('valid_batch_'+metric_name, value)
+
+        return {'loss': loss, **metrics}
 
 
     def validation_epoch_end(self, outputs):
@@ -68,16 +124,16 @@ class Classifier(pl.LightningModule):
 
     def test_step(self, batch, batch_idx):
         x,y = batch
-        # x = x.view(x.size(0), -1)
         pred = self.model(x)
         loss = self.loss_func(pred, y)
+        self.log('test_batch_loss', loss)
 
         pred = torch.argmax(pred, dim=1)
-        metrics = self.compute_metrics(pred, y, 'test')
+        metrics = self.compute_metrics(pred, y)
         for metric_name, value in metrics.items():
-            self.log('test_'+metric_name+'_batch', value)
+            self.log('test_batch_'+metric_name, value)
 
-        return metrics
+        return {'loss': loss, **metrics}
 
     def test_epoch_end(self, outputs):
         means = {'test_epoch_'+name: 0 for name in outputs[0].keys()}
@@ -91,7 +147,32 @@ class Classifier(pl.LightningModule):
 
         return means
 
+############### Other #####################
+
     def configure_optimizers(self):
         optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer],[lr_scheduler]
+
+
+    def _split_df(self, annotation_file, split_dataset, random_state) -> Tuple[DataLoader,DataLoader, DataLoader]:
+
+        df = CustomDataset.read_annotation_file(annotation_file)
+
+        necessary_columns = ('name','class')
+        for column_name in necessary_columns:
+            assert column_name in df.columns, f"Columns: {necessary_columns} should be in the dataframe"
+
+        if split_dataset:
+            df_train, df_valid, df_test = CustomDataset.split_df(df, random_state)
+        else:
+            assert 'split' in df.columns
+            df_train = df[df['split'] == 'train']
+            df_valid = df[df['split'] == 'valid']
+            df_test = df[df['split'] == 'test']
+        return df_train, df_valid, df_test
+
+    
+    def compute_metrics(self, pred, y):
+        accuracy = torch.sum(y == pred).item() / (len(y) * 1.0)
+        return {'accuracy': accuracy}
\ No newline at end of file
diff --git a/model/wandb/debug-internal.log b/model/wandb/debug-internal.log
index 336d471..8aabcd6 120000
--- a/model/wandb/debug-internal.log
+++ b/model/wandb/debug-internal.log
@@ -1 +1 @@
-run-20220523_005215-2f20jaiw/logs/debug-internal.log
\ No newline at end of file
+run-20220523_190540-lkrzgzcr/logs/debug-internal.log
\ No newline at end of file
diff --git a/model/wandb/debug.log b/model/wandb/debug.log
index 5b3df1f..83befcf 120000
--- a/model/wandb/debug.log
+++ b/model/wandb/debug.log
@@ -1 +1 @@
-run-20220523_005215-2f20jaiw/logs/debug.log
\ No newline at end of file
+run-20220523_190540-lkrzgzcr/logs/debug.log
\ No newline at end of file
diff --git a/model/wandb/latest-run b/model/wandb/latest-run
index 0f3c85c..9332a28 120000
--- a/model/wandb/latest-run
+++ b/model/wandb/latest-run
@@ -1 +1 @@
-run-20220523_005215-2f20jaiw
\ No newline at end of file
+run-20220523_190540-lkrzgzcr
\ No newline at end of file
